{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Week 6:** Model Building\n",
    "### Neural Network, Random Forest, Gradient Boosting\n",
    "#### July 27, 2023\n",
    "---------------- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an exercise in preparing and building neural network, random forest, and gradient boosting models using the Ames Iowa housing data (`ames_housing.csv`). \n",
    "\n",
    "[Link to Ames Housing Kaggle Dataset](https://www.kaggle.com/datasets/marcopale/housing)\n",
    "\n",
    "[Link to Titanic Training Dataset](https://www.kaggle.com/competitions/titanic/data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "import category_encoders as ce\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model - Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the `ames_housing.csv` CSV file into a Pandas Dataframe, and call it `ames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames = pd.read_csv('ames_housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the target and predictor variables for the Ames dataset. We want to predict the sale price for each home observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames.drop(['Sale_Price'], axis = 1)\n",
    "y = ames['Sale_Price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets (70%, 30%). Print out the number of rows in each set to make sure the data is split correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the head of the dataset to preview the columns and column types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce down the number of variables only for ease of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, ['Bedroom_AbvGr', 'Year_Built', 'Mo_Sold', 'Lot_Area', \n",
    "           'Street', 'Central_Air', '1st_Flr_SF', '2nd_Flr_SF', 'Full_Bath', \n",
    "           'Half_Bath', 'Fireplaces', 'Garage_Area', 'Gr_Liv_Area', 'TotRms_AbvGrd']] \n",
    "X_test = X_test.loc[:, ['Bedroom_AbvGr', 'Year_Built', 'Mo_Sold', 'Lot_Area', \n",
    "           'Street', 'Central_Air', '1st_Flr_SF', '2nd_Flr_SF', 'Full_Bath', \n",
    "           'Half_Bath', 'Fireplaces', 'Garage_Area', 'Gr_Liv_Area', 'TotRms_AbvGrd']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the number of unique values in each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Random Forest (Regressor) library in Sci-Kit Learn, and this package does not accept string variables. We need to encode the `Street` and `Central_Air` variables as a number for each unique value. Let's use the `category_encoders` package to do this. Output the head of the dataset again to make sure the encoding worked properly (i.e., all variables in the dataframe should be numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder(cols=['Street', 'Central_Air'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instantiate the RandomForestRegressor model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model we just instantiated on our feature engineered training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for the sale price of the homes in the test dataset using the model we just fit to our training dataset.\n",
    "\n",
    "Output the MAE and MAPE for the model predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = fit_rf.predict(X_test)\n",
    "print(\"MAE: \", sklearn.metrics.mean_absolute_error(y_test, preds))\n",
    "print(\"MAPE: \", sklearn.metrics.mean_absolute_percentage_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try different numbers for the parameter `n_estimators`, a.k.a. \"tuning\" the model. Create an array of numbers to try from 10 to 200 in increments of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = np.arange(10, 200, 10)\n",
    "estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit the model again using each of the numbers in the `estimators` array. This code may take a minute to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for n in estimators:\n",
    "    fit_rf.set_params(n_estimators=n)\n",
    "    fit_rf.fit(X_train, y_train)\n",
    "    scores.append(fit_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the value of `n_estimator` against the score of the tuned model for each value of the parameter. We can see that the effect of `n_estimators` on the score of the model levels out around 125, so this might be a good setting for the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Effect of n_estimators\")\n",
    "plt.xlabel(\"n_estimator\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.plot(estimators, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can output the raw scores to make your decision for tuning the `n_estimator` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the same training and test datasets for Ames housing we used for the Random Forest model. We will run a grid search across 5 different parameters: `max_depth`, `learning_rate`, `gamma`, `reg_lambda`, and `scale_pos_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth' :[3,4,5],\n",
    "    'learning_rate':[0.1, 0.01, 0.5],\n",
    "    'gamma':[0,0.25,1],\n",
    "    'reg_lambda':[0, 1.0, 10.0],\n",
    "    'scale_pos_weight':[1,3,5]\n",
    "}\n",
    "\n",
    "optimal_params = GridSearchCV(estimator = xgb.XGBRegressor(subsample=0.9, colsample_bytree=0.5), param_grid = param_grid, verbose = 0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model we instantiated using the Grid Search Cross-Validation method on our feature engineered training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_model = optimal_params.fit(X_train, y_train, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for the sale price of the homes in the test dataset using the model we just fit to our training dataset.\n",
    "\n",
    "Output the MAE and MAPE for the model predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gbm_model.predict(X_test)\n",
    "print(\"MAE: \", sklearn.metrics.mean_absolute_error(y_test, preds))\n",
    "print(\"MAPE: \", sklearn.metrics.mean_absolute_percentage_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instantiate the MLPRegressor (Neural Network) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet = MLPRegressor(hidden_layer_sizes=(2000, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model we just instantiated on our feature engineered training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnet_model = nnet.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for the sale price of the homes in the test dataset using the model we just fit to our training dataset.\n",
    "\n",
    "Output the MAE and MAPE for the model predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nnet_model.predict(X_test)\n",
    "print(\"MAE: \", sklearn.metrics.mean_absolute_error(y_test, preds))\n",
    "print(\"MAPE: \", sklearn.metrics.mean_absolute_percentage_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the `titanic.csv` CSV file into a Pandas Dataframe, and call it `titanic.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the target and predictor variables for the Titanic dataset. We want to classify observations on whether or not they survived the Titanic sinking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.drop(['Survived'], axis = 1)\n",
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and test sets for both X and y (70%, 30%). Print out the number of rows in each set to make sure the data is split correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the head of the dataset to preview the columns and column types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Selection and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove the `PassengerId`, `Name`, and `Ticket` variables because they are not useful to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:, ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']] \n",
    "X_test = X_test.loc[:, ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the number of unique values in each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce the number of unique values in the `Cabin` variable by categorizing each passenger by the first letter of their Cabin (e.g., `B77` would become `B`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Cabin'] = X_train['Cabin'].str[0]\n",
    "X_test['Cabin'] = X_test['Cabin'].str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the decision tree library in Sci-Kit Learn, and this package does not accept string variables. We need to encode the `Sex`, `Cabin`, `Age`, and `Embarked` variables as a number for each unique value. Let's use the `category_encoders` package to do this. Output the head of the dataset again to make sure the encoding worked properly (i.e., all variables in the dataframe should be numeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OrdinalEncoder(cols=['Sex', 'Cabin', 'Age', 'Embarked'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "X_test = encoder.transform(X_test)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, instantiate the DecisionTreeClassifier model. Let's use the Gini index as the class target criterion. Feel free to come back and switch this criterion to another metric later and compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the model we just instantiated on our feature engineered training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make predictions for survival of the passengers in the test dataset using the model we just fit to our training dataset. Set the array of predicted values (0s or 1s) to the variable `y_pred_gini.`\n",
    "\n",
    "Get predictions for both the test data and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gini = clf_gini.predict(X_test)\n",
    "# y_pred_gini\n",
    "\n",
    "y_pred_train_gini = clf_gini.predict(X_train)\n",
    "# y_pred_train_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compare the accuracy of the training set predictions with the accuracy of the test set predictions to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set score: {:.4f}'.format(sklearn.metrics.accuracy_score(y_train, y_pred_train_gini)))\n",
    "print('Test set score: {:.4f}'.format(sklearn.metrics.accuracy_score(y_test, y_pred_gini)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the confusion matrix for the model. Remember the confusion matrix should be made up of the following:\n",
    "\n",
    "<img src=\"cm.png\" width=500 height=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_gini)\n",
    "print('Confusion matrix\\n\\n', cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization and Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's output a visual for the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "tree.plot_tree(clf_gini.fit(X_train, y_train)) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
